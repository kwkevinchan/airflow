# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.
#
# WARNING: This configuration is for local development. Do not use it in a production deployment.
#
# This configuration supports basic configuration using environment variables or an .env file
# The following variables are supported:
#
# AIRFLOW_IMAGE_NAME           - Docker image name used to run Airflow.
#                                Default: apache/airflow:master-python3.8
# AIRFLOW_UID                  - User ID in Airflow containers
#                                Default: 50000
# AIRFLOW_GID                  - Group ID in Airflow containers
#                                Default: 0
#
# Those configurations are useful mostly in case of standalone testing/running Airflow in test/try-out mode
#
# _AIRFLOW_WWW_USER_USERNAME   - Username for the administrator account (if requested).
#                                Default: airflow
# _AIRFLOW_WWW_USER_PASSWORD   - Password for the administrator account (if requested).
#                                Default: airflow
# _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all containers.
#                                Default: ''
#
# Feel free to modify this file to suit your needs.
---
version: '3.7'

networks:
  traefik_bridge:
    external: true
    name: traefik_bridge

x-airflow-common:
  &airflow-common
  # In order to add custom dependencies or upgrade provider packages you can use your extended image.
  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
  # and uncomment the "build" line below, Then run `docker-compose build` to build the images.
  image: apache/airflow:2.1.3-python3.8
  # build: .
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: mysql+mysqlconnector://airflow:m71qPQl1joNS1857ziNurKO5ZfMfZikD@mysql:3306/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+mysql://airflow:m71qPQl1joNS1857ziNurKO5ZfMfZikD@mysql:3306/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: '2J7OGHGRHoE0BPWZDRdt1hRfuh8Zi83S7IKa2HeiKc4='
    # python
    # from cryptography.fernet import Fernet
    # fernet_key= Fernet.generate_key()
    # print(fernet_key.decode()) # your fernet_key, keep it in secured place!
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__API__AUTH_BACKEND: 'airflow.api.auth.backend.basic_auth'
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    AIRFLOW_UID: 50000
  user: "50000:0"
  volumes:
  - ./logs/:/opt/airflow/logs
  - ./dags/:/opt/airflow/dags
  - ./plugins/:/opt/airflow/plugins
  - ./airflow.cfg:/opt/airflow/airflow.cfg
  - ./webserver_config.py:/opt/airflow/webserver_config.py


services:
  webserver:
    image: apache/airflow:2.1.3-python3.8
    command: webserver
    environment:
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: mysql+mysqlconnector://airflow:m71qPQl1joNS1857ziNurKO5ZfMfZikD@mysql:3306/airflow
    volumes:
      - ./logs/:/opt/airflow/logs
      - ./airflow.cfg:/opt/airflow/airflow.cfg
      - ./webserver_config.py:/opt/airflow/webserver_config.py
    networks:
      - traefik_bridge
    labels: 
      - "traefik.http.routers.airflow-webserver-${USER}.rule=Host(`${USER}-airflow-webserver-traefik.dev.kuobrothers.com`)"
      - "traefik.http.services.airflow-webserver-${USER}.loadbalancer.server.port=8080"
      - "traefik.enable=true"

  scheduler:
    image: apache/airflow:2.1.3-python3.8
    command: scheduler
    volumes:
      - ./logs/:/opt/airflow/logs
      - ./dags/:/opt/airflow/dags
      - ./plugins/:/opt/airflow/plugins
      - ./airflow.cfg:/opt/airflow/airflow.cfg
      - ./webserver_config.py:/opt/airflow/webserver_config.py
    environment:
      <<: *airflow-common-env
    networks:
      - traefik_bridge

  worker:
    image: apache/airflow:2.1.3-python3.8
    environment:
      <<: *airflow-common-env
    user: "50000:0"
    command: celery worker
    volumes:
      - ./logs/:/opt/airflow/logs
      - ./airflow.cfg:/opt/airflow/airflow.cfg
      - ./webserver_config.py:/opt/airflow/webserver_config.py
    networks:
      - traefik_bridge
  
  workspace:
    image: apache/airflow:2.1.3-python3.8
    environment:
      <<: *airflow-common-env
    user: "50000:0"
    volumes: 
      - ./logs/:/opt/airflow/logs
      - ./dags/:/opt/airflow/dags
      - ./plugins/:/opt/airflow/plugins
      - ./airflow.cfg:/opt/airflow/airflow.cfg
      - ./webserver_config.py:/opt/airflow/webserver_config.py
    entrypoint: tail -f /dev/null
    networks:
      - traefik_bridge

  # flower:
  #   <<: *airflow-common
  #   command: celery flower
  #   healthcheck:
  #     test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
  #     interval: 10s
  #     timeout: 10s
  #     retries: 5
  #   restart: always
  #   labels: 
  #     - "traefik.http.routers.celery-flower-${USER}.rule=Host(`flower-traefik.test`)"
  #     - "traefik.http.services.celery-flower-${USER}.loadbalancer.server.port=5555"
  #     - "traefik.enable=true"
  #   networks:
  #     - traefik_bridge

### DataBase ###
  mysql:
    image: mysql:5.7
    environment:
      MYSQL_ROOT_PASSWORD: XxnMYTsUBCAwdyEztE8ygLkbHMitSk9r
      MYSQL_USER: airflow
      MYSQL_PASSWORD: m71qPQl1joNS1857ziNurKO5ZfMfZikD
      MYSQL_DATABASE: airflow
    volumes:
      - ./data/mysql/:/var/lib/mysql
      - ./mysql/my.cnf:/etc/mysql/conf.d/mysql-docker.cnf
    networks:
      - traefik_bridge

  redis:
    image: redis:6.2.5-alpine
    networks:
      - traefik_bridge
